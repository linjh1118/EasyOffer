{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/yf/fpk8smns2ws1yhd_1vhg7td80000gn/T/ipykernel_35798/2673204285.py\", line 6, in <module>\n",
      "    import torch\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/jt/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/jt/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/jt/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/jt/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10a814f30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# DPO（Direct Preference Optimization）算法实现\n",
    "# DPO通过人类偏好数据直接优化语言模型，使其生成更符合人类偏好的输出\n",
    "# 这里面使用了一个偏好prefer以及两个reject的格式\n",
    "# ===============================================================================\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(10, 32)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (k_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (v_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=32, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=32, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=32, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((32,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((32,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((32,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型\n",
    "# 创建简化版的Llama模型作为策略模型（将被优化的模型）\n",
    "policy_model = LlamaForCausalLM(config=LlamaConfig(vocab_size=12, num_hidden_layers=1, hidden_size=32))\n",
    "# 创建参考模型（通常是SFT模型，在训练过程中保持不变）\n",
    "reference_model = deepcopy(policy_model)  # 深度复制确保两个模型初始参数完全相同\n",
    "policy_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "beta = 0.1  # DPO的温度系数，控制策略模型与参考模型的偏离程度，值越小允许偏离越大\n",
    "\n",
    "# 准备训练数据\n",
    "# 在DPO中，我们需要提示(prompt)、优选回答(chosen/good)和拒绝回答(rejected/bad)\n",
    "prompt_ids = [1, 2, 3, 4, 5, 6]  # 输入提示的token IDs\n",
    "good_response_ids = [7, 8, 9, 2]  # 优质回答的token IDs\n",
    "# 多个低质量回答的示例，每个都是token IDs的列表\n",
    "bad_response_ids_list = [[1, 2, 0, 0], [4, 5, 6, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 2],\n",
       "        [1, 2, 3, 4, 5, 6, 1, 2, 0, 0],\n",
       "        [1, 2, 3, 4, 5, 6, 4, 5, 6, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建模型输入：将提示与回答拼接\n",
    "# 创建包含多个序列的批次：[提示+优质回答, 提示+低质回答1, 提示+低质回答2, ...]\n",
    "input_ids = torch.LongTensor(\n",
    "    [prompt_ids + good_response_ids, *[prompt_ids + bad_response_ids for bad_response_ids in bad_response_ids_list]]\n",
    ")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100,    7,    8,    9,    2],\n",
       "        [-100, -100, -100, -100, -100, -100,    1,    2,    0,    0],\n",
       "        [-100, -100, -100, -100, -100, -100,    4,    5,    6,    0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备用于计算语言模型损失的标签\n",
    "# 在语言模型训练中，标签是输入向右移动一位（预测下一个token）\n",
    "# -100表示在计算损失时忽略该位置（这里忽略提示部分）\n",
    "labels = torch.LongTensor(\n",
    "    [\n",
    "        [-100] * len(prompt_ids) + good_response_ids,\n",
    "        *[[-100] * len(prompt_ids) + bad_response_ids for bad_response_ids in bad_response_ids_list]\n",
    "    ]\n",
    ") # 向右移动一位，因为我们预测的是下一个token\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100,    7,    8,    9,    2],\n",
       "        [-100, -100, -100, -100, -100,    1,    2,    0,    0],\n",
       "        [-100, -100, -100, -100, -100,    4,    5,    6,    0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labels[:, 1:]  \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建掩码，用于标识哪些位置参与损失计算（即回答部分）\n",
    "loss_mask = (labels != -100)\n",
    "print(loss_mask.shape)\n",
    "loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 7, 8, 9, 2],\n",
       "        [0, 0, 0, 0, 0, 1, 2, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 4, 5, 6, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将-100替换为0，因为在gather操作中-100是无效索引\n",
    "labels[labels == -100] = 0\n",
    "print(labels.shape)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([3, 10, 10])\n",
      "past_key_values: <class 'transformers.cache_utils.DynamicCache'>\n"
     ]
    }
   ],
   "source": [
    "output = policy_model(input_ids)\n",
    "for key, value in output.items():  # 如果是ModelOutput对象，可以用output.__dict__.items()\n",
    "    if hasattr(value, \"shape\"):\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0565, -0.0827, -0.1474,  0.0100, -0.0097, -0.1095,  0.0679,\n",
       "           0.0035, -0.0819, -0.0010],\n",
       "         [ 0.1082, -0.1835, -0.1182, -0.1179,  0.0160, -0.1471,  0.0077,\n",
       "          -0.1568, -0.0925,  0.0218],\n",
       "         [ 0.0287, -0.1253,  0.0064, -0.0516, -0.0354, -0.0353, -0.0429,\n",
       "          -0.2031, -0.0248, -0.0239],\n",
       "         [-0.0219,  0.1550,  0.0996,  0.1397, -0.1033,  0.0055, -0.1241,\n",
       "           0.0612,  0.0729, -0.0935],\n",
       "         [-0.1054,  0.0074, -0.1446, -0.1313, -0.0460,  0.0428,  0.0211,\n",
       "           0.0946,  0.1309, -0.0070],\n",
       "         [ 0.0928, -0.1962, -0.0238, -0.0858, -0.1913, -0.2821, -0.0004,\n",
       "          -0.1976,  0.0196, -0.1160],\n",
       "         [-0.1436,  0.0744, -0.0164,  0.0534,  0.0762, -0.0857, -0.2603,\n",
       "          -0.0457,  0.1537,  0.0267],\n",
       "         [-0.1182,  0.0489, -0.0797,  0.0039, -0.0279,  0.1024,  0.0110,\n",
       "           0.0005, -0.0170, -0.0856],\n",
       "         [-0.0509, -0.0215, -0.0647,  0.0435, -0.0992,  0.0381, -0.1139,\n",
       "           0.0671, -0.0092, -0.1155]],\n",
       "\n",
       "        [[-0.0565, -0.0827, -0.1474,  0.0100, -0.0097, -0.1095,  0.0679,\n",
       "           0.0035, -0.0819, -0.0010],\n",
       "         [ 0.1082, -0.1835, -0.1182, -0.1179,  0.0160, -0.1471,  0.0077,\n",
       "          -0.1568, -0.0925,  0.0218],\n",
       "         [ 0.0287, -0.1253,  0.0064, -0.0516, -0.0354, -0.0353, -0.0429,\n",
       "          -0.2031, -0.0248, -0.0239],\n",
       "         [-0.0219,  0.1550,  0.0996,  0.1397, -0.1033,  0.0055, -0.1241,\n",
       "           0.0612,  0.0729, -0.0935],\n",
       "         [-0.1054,  0.0074, -0.1446, -0.1313, -0.0460,  0.0428,  0.0211,\n",
       "           0.0946,  0.1309, -0.0070],\n",
       "         [ 0.0928, -0.1962, -0.0238, -0.0858, -0.1913, -0.2821, -0.0004,\n",
       "          -0.1976,  0.0196, -0.1160],\n",
       "         [-0.0445, -0.0359, -0.1230, -0.0629, -0.0283, -0.1006,  0.0080,\n",
       "           0.0279, -0.1417,  0.0006],\n",
       "         [ 0.0762, -0.1421, -0.0977, -0.1530, -0.0307, -0.1572, -0.0045,\n",
       "          -0.1844, -0.1458,  0.0421],\n",
       "         [ 0.1235, -0.1921,  0.1505,  0.1238,  0.0671, -0.1188,  0.0918,\n",
       "          -0.1072,  0.0289, -0.0068]],\n",
       "\n",
       "        [[-0.0565, -0.0827, -0.1474,  0.0100, -0.0097, -0.1095,  0.0679,\n",
       "           0.0035, -0.0819, -0.0010],\n",
       "         [ 0.1082, -0.1835, -0.1182, -0.1179,  0.0160, -0.1471,  0.0077,\n",
       "          -0.1568, -0.0925,  0.0218],\n",
       "         [ 0.0287, -0.1253,  0.0064, -0.0516, -0.0354, -0.0353, -0.0429,\n",
       "          -0.2031, -0.0248, -0.0239],\n",
       "         [-0.0219,  0.1550,  0.0996,  0.1397, -0.1033,  0.0055, -0.1241,\n",
       "           0.0612,  0.0729, -0.0935],\n",
       "         [-0.1054,  0.0074, -0.1446, -0.1313, -0.0460,  0.0428,  0.0211,\n",
       "           0.0946,  0.1309, -0.0070],\n",
       "         [ 0.0928, -0.1962, -0.0238, -0.0858, -0.1913, -0.2821, -0.0004,\n",
       "          -0.1976,  0.0196, -0.1160],\n",
       "         [-0.0497,  0.1791,  0.0838,  0.1562, -0.0954,  0.0160, -0.0987,\n",
       "           0.0749,  0.0925, -0.0861],\n",
       "         [-0.1003, -0.0116, -0.1442, -0.1418, -0.0371,  0.0671,  0.0496,\n",
       "           0.0896,  0.1160,  0.0103],\n",
       "         [ 0.0825, -0.1899, -0.0342, -0.0874, -0.1939, -0.2715,  0.0080,\n",
       "          -0.1895,  0.0141, -0.1027]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# 计算策略模型（policy model）的对数概率\n",
    "# ===============================================================================\n",
    "# 前向传播，获取每个token位置的预测logits\n",
    "logits = policy_model(input_ids)[\"logits\"][:, :-1, :]  # 去掉最后一个位置，与label对齐\n",
    "print(logits.shape)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.7857, -6.6947, -6.7697, -6.5169, -6.6811, -7.0314, -6.7662, -6.5559,\n",
       "         -6.6359],\n",
       "        [-6.7857, -6.6947, -6.7697, -6.5169, -6.6811, -7.1295, -6.9317, -6.6988,\n",
       "         -6.4697],\n",
       "        [-6.7857, -6.6947, -6.7697, -6.5169, -6.6811, -6.7405, -6.8247, -7.2198,\n",
       "         -6.8045]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将logits转换为对数概率，并提取每个位置上正确token的对数概率\n",
    "per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "per_token_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-60.4376, -60.6779, -61.0376], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 仅对回答部分（loss_mask=True的位置）求和，得到每个序列的总对数概率\n",
    "all_logps = (per_token_logps * loss_mask).sum(-1)\n",
    "all_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分离优质回答和低质量回答的对数概率\n",
    "policy_good_logps, policy_bad_logps = all_logps[:1], all_logps[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-60.4376], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_good_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-60.6779, -61.0376], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_bad_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9, 10])\n",
      "logits:\n",
      " tensor([[[-0.0565, -0.0827, -0.1474,  0.0100, -0.0097, -0.1095,  0.0679,\n",
      "           0.0035, -0.0819, -0.0010],\n",
      "         [ 0.1082, -0.1835, -0.1182, -0.1179,  0.0160, -0.1471,  0.0077,\n",
      "          -0.1568, -0.0925,  0.0218],\n",
      "         [ 0.0287, -0.1253,  0.0064, -0.0516, -0.0354, -0.0353, -0.0429,\n",
      "          -0.2031, -0.0248, -0.0239],\n",
      "         [-0.0219,  0.1550,  0.0996,  0.1397, -0.1033,  0.0055, -0.1241,\n",
      "           0.0612,  0.0729, -0.0935],\n",
      "         [-0.1054,  0.0074, -0.1446, -0.1313, -0.0460,  0.0428,  0.0211,\n",
      "           0.0946,  0.1309, -0.0070],\n",
      "         [ 0.0928, -0.1962, -0.0238, -0.0858, -0.1913, -0.2821, -0.0004,\n",
      "          -0.1976,  0.0196, -0.1160],\n",
      "         [-0.1436,  0.0744, -0.0164,  0.0534,  0.0762, -0.0857, -0.2603,\n",
      "          -0.0457,  0.1537,  0.0267],\n",
      "         [-0.1182,  0.0489, -0.0797,  0.0039, -0.0279,  0.1024,  0.0110,\n",
      "           0.0005, -0.0170, -0.0856],\n",
      "         [-0.0509, -0.0215, -0.0647,  0.0435, -0.0992,  0.0381, -0.1139,\n",
      "           0.0671, -0.0092, -0.1155]],\n",
      "\n",
      "        [[-0.0565, -0.0827, -0.1474,  0.0100, -0.0097, -0.1095,  0.0679,\n",
      "           0.0035, -0.0819, -0.0010],\n",
      "         [ 0.1082, -0.1835, -0.1182, -0.1179,  0.0160, -0.1471,  0.0077,\n",
      "          -0.1568, -0.0925,  0.0218],\n",
      "         [ 0.0287, -0.1253,  0.0064, -0.0516, -0.0354, -0.0353, -0.0429,\n",
      "          -0.2031, -0.0248, -0.0239],\n",
      "         [-0.0219,  0.1550,  0.0996,  0.1397, -0.1033,  0.0055, -0.1241,\n",
      "           0.0612,  0.0729, -0.0935],\n",
      "         [-0.1054,  0.0074, -0.1446, -0.1313, -0.0460,  0.0428,  0.0211,\n",
      "           0.0946,  0.1309, -0.0070],\n",
      "         [ 0.0928, -0.1962, -0.0238, -0.0858, -0.1913, -0.2821, -0.0004,\n",
      "          -0.1976,  0.0196, -0.1160],\n",
      "         [-0.0445, -0.0359, -0.1230, -0.0629, -0.0283, -0.1006,  0.0080,\n",
      "           0.0279, -0.1417,  0.0006],\n",
      "         [ 0.0762, -0.1421, -0.0977, -0.1530, -0.0307, -0.1572, -0.0045,\n",
      "          -0.1844, -0.1458,  0.0421],\n",
      "         [ 0.1235, -0.1921,  0.1505,  0.1238,  0.0671, -0.1188,  0.0918,\n",
      "          -0.1072,  0.0289, -0.0068]],\n",
      "\n",
      "        [[-0.0565, -0.0827, -0.1474,  0.0100, -0.0097, -0.1095,  0.0679,\n",
      "           0.0035, -0.0819, -0.0010],\n",
      "         [ 0.1082, -0.1835, -0.1182, -0.1179,  0.0160, -0.1471,  0.0077,\n",
      "          -0.1568, -0.0925,  0.0218],\n",
      "         [ 0.0287, -0.1253,  0.0064, -0.0516, -0.0354, -0.0353, -0.0429,\n",
      "          -0.2031, -0.0248, -0.0239],\n",
      "         [-0.0219,  0.1550,  0.0996,  0.1397, -0.1033,  0.0055, -0.1241,\n",
      "           0.0612,  0.0729, -0.0935],\n",
      "         [-0.1054,  0.0074, -0.1446, -0.1313, -0.0460,  0.0428,  0.0211,\n",
      "           0.0946,  0.1309, -0.0070],\n",
      "         [ 0.0928, -0.1962, -0.0238, -0.0858, -0.1913, -0.2821, -0.0004,\n",
      "          -0.1976,  0.0196, -0.1160],\n",
      "         [-0.0497,  0.1791,  0.0838,  0.1562, -0.0954,  0.0160, -0.0987,\n",
      "           0.0749,  0.0925, -0.0861],\n",
      "         [-0.1003, -0.0116, -0.1442, -0.1418, -0.0371,  0.0671,  0.0496,\n",
      "           0.0896,  0.1160,  0.0103],\n",
      "         [ 0.0825, -0.1899, -0.0342, -0.0874, -0.1939, -0.2715,  0.0080,\n",
      "          -0.1895,  0.0141, -0.1027]]])\n",
      "torch.Size([3, 9])\n",
      "per_token_logps:\n",
      " tensor([[-2.3203, -2.1325, -2.2251, -2.3483, -2.3982, -2.4085, -2.1387, -2.3740,\n",
      "         -2.3367],\n",
      "        [-2.3203, -2.1325, -2.2251, -2.3483, -2.3982, -2.4071, -2.3770, -2.1506,\n",
      "         -2.2013],\n",
      "        [-2.3203, -2.1325, -2.2251, -2.3483, -2.3982, -2.4022, -2.3188, -2.2468,\n",
      "         -2.1295]])\n",
      "torch.Size([3])\n",
      "all_logps\n",
      " tensor([-9.2579, -9.1361, -9.0972])\n",
      "torch.Size([1])\n",
      "reference_good_logps:\n",
      " tensor([-9.2579])\n",
      "torch.Size([2])\n",
      "reference_bad_logps\n",
      " tensor([-9.1361, -9.0972])\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# 计算参考模型（reference model）的对数概率\n",
    "# ===============================================================================\n",
    "with torch.no_grad():  # 不计算梯度，因为参考模型不需要更新\n",
    "    # 重复与策略模型相同的步骤\n",
    "    logits = reference_model(input_ids)[\"logits\"][:, :-1, :]\n",
    "    print(logits.shape)\n",
    "    print(\"logits:\\n\",logits)\n",
    "    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "    print(per_token_logps.shape)\n",
    "    print(\"per_token_logps:\\n\",per_token_logps)\n",
    "    all_logps = (per_token_logps * loss_mask).sum(-1)\n",
    "    print(all_logps.shape)\n",
    "    print(\"all_logps\\n\",all_logps)\n",
    "    reference_good_logps, reference_bad_logps = all_logps[:1], all_logps[1:]\n",
    "    print(reference_good_logps.shape)\n",
    "    print(\"reference_good_logps:\\n\",reference_good_logps)\n",
    "    print(reference_bad_logps.shape)\n",
    "    print(\"reference_bad_logps\\n\",reference_bad_logps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# 计算DPO损失\n",
    "# DPO的核心思想：增大策略模型对优质回答的概率，同时减小对低质量回答的概率\n",
    "# ===============================================================================\n",
    "# 计算DPO的logits：(策略模型相对于参考模型对好回答的提升) - (对坏回答的提升)\n",
    "logits = (policy_good_logps - reference_good_logps) - (policy_bad_logps - reference_bad_logps)\n",
    "# 应用logsigmoid函数并乘以beta控制优化强度，取负值（因为要最小化损失）\n",
    "loss = -F.logsigmoid(beta * logits).mean()  # 对所有样本取平均\n",
    "\n",
    "# 输出损失值\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
